# LLM configuration (LiteLLM OpenAI-compatible proxy)
#
# OPENAI_API_KEY
#   - API key used by LiteLLM/OpenAI-compatible proxy.
#   - Required if your proxy or provider enforces authentication.
#   - No default; must be set explicitly for real runs.
OPENAI_API_KEY=your-litellm-key

# OPENAI_BASE_URL
#   - Base URL for the LiteLLM/OpenAI-compatible proxy.
#   - Example: local LiteLLM proxy running on port 4000.
#   - Default (if unset): picked up by LiteLLM according to its own rules.
OPENAI_BASE_URL=http://localhost:4000

#
# SymPrompt / evolution LLM model selection
#
# SYMPROMPT_LLM_MODEL
#   - Primary model name used by SymPrompt and all evolution runners.
#   - Read in symprompt/evolution/run_*_evolution.py, then passed to OpenEvolve.
#   - REQUIRED for all evolution runs; if unset, the runners will raise an error.
#   - Example: SYMPROMPT_LLM_MODEL=lm_studio/primeintellect_intellect-3
#
# EVOLVE_LLM_MODEL
#   - Convenience alias used ONLY by scripts/run_*_evolution.py.
#   - If set and SYMPROMPT_LLM_MODEL is NOT already set, the script copies
#     EVOLVE_LLM_MODEL into SYMPROMPT_LLM_MODEL for that process.
#   - If SYMPROMPT_LLM_MODEL is already set, EVOLVE_LLM_MODEL is ignored.
EVOLVE_LLM_MODEL=

#
# LLM timeout / retry configuration for long evolution runs
#
# SYMPROMPT_LLM_TIMEOUT_SECONDS
#   - Per-call timeout (seconds) for LiteLLM sync client used by evolution.
#   - Overrides DEFAULT_LLM_CONFIG.timeout_seconds when set.
#   - Default (if unset): 900 seconds.
SYMPROMPT_LLM_TIMEOUT_SECONDS=

# SYMPROMPT_LLM_RETRIES
#   - Number of retries for LiteLLM sync client before returning an empty response.
#   - Overrides DEFAULT_LLM_CONFIG.retries when set.
#   - Default (if unset): 10.
SYMPROMPT_LLM_RETRIES=

#
# Evaluation / evolution control
#
# EVAL_PARALLEL_BENCHMARKS
#   - Maximum number of benchmarks evaluated in parallel inside EACH worker process.
#   - Used by evaluate_fast (Phase 1) and eval_pipeline_phase2 (Phase 2).
#   - The code caps per-process threads at 8 for robustness:
#       effective_threads = max(1, min(EVAL_PARALLEL_BENCHMARKS, 8))
#   - Default (if unset): 16 (capped to 8 internally).
EVAL_PARALLEL_BENCHMARKS=

# EVAL_SHOW_PROGRESS
#   - Whether to print per-benchmark progress during evaluation.
#   - "1", "true", "yes" -> enabled; anything else -> disabled.
#   - Default (if unset): "1".
EVAL_SHOW_PROGRESS=

# EVAL_INCLUDE_WILD
#   - When set to "1"/"true"/"yes", include wild_prompts.json benchmarks
#     in evaluation to reduce overfitting.
#   - Used by both Phase 1 and Phase 2 evaluators.
#   - Default (if unset): "0" (wild prompts disabled for speed).
EVAL_INCLUDE_WILD=

#
# Evolution runner convenience flags (read by scripts/run_*_evolution.py)
#
# EVOLVE_ITERATIONS
#   - Number of iterations for evolution runs.
#   - Default (if unset): value from openevolve_config.yaml -> max_iterations.
EVOLVE_ITERATIONS=

# EVOLVE_RESUME
#   - If set to "1"/"true"/"yes", resume from the last evolution.db checkpoint.
#   - Default (if unset): start a fresh evolution run (with backup + seeds).
EVOLVE_RESUME=

# EVOLVE_MODE
#   - Evolution mode for translation pipeline:
#       "phase1" -> Tier 1 focus (fast evaluator, eval_pipeline.py)
#       "phase2" -> full system (router + escalation + domain-weighted accuracy,
#                   eval_pipeline_phase2.py)
#   - Used by scripts/run_translation_evolution.py only.
#   - Default (if unset): "phase1".
EVOLVE_MODE=
